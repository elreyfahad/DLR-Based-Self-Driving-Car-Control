{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "#from dqn import logger\n",
    "#from dqn.commons.schedules import LinearSchedule\n",
    "from mlagents.envs import UnityEnvironment\n",
    "#from pg_actor_critic import PolicyGradientActorCritic\n",
    "#from __future__ import print_function\n",
    "from collections import deque\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment path\n",
    "\n",
    "Be sure to set `env_name` to the name of the Unity environment file you want to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_ = \"Windows\"\n",
    "\n",
    "env_name = \"../environment/\" + os_ + \"/Driving\" # Name of the Unity environment binary to launch\n",
    "train_mode = True # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the environment\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_name,worker_id=2, base_port=5005)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]\n",
    "#brain.vector_action_space_type='continuous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_actor_network(state_dim,num_actions):\n",
    "    \n",
    "    # define policy neural network\n",
    "    def network_fn(states):\n",
    "        W1 = tf.get_variable(\"W1\", [state_dim, 20],initializer=tf.random_normal_initializer())\n",
    "        b1 = tf.get_variable(\"b1\", [20],initializer=tf.constant_initializer(0))\n",
    "        h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n",
    "        W2 = tf.get_variable(\"W2\", [20, num_actions],initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        b2 = tf.get_variable(\"b2\", [num_actions],initializer=tf.constant_initializer(0))\n",
    "        p = tf.matmul(h1, W2) + b2\n",
    "\n",
    "        return p\n",
    "    return network_fn\n",
    "\n",
    "def build_critic_network(state_dim,num_actions):\n",
    "    \n",
    "    def network_fn(states):\n",
    "        # define policy neural network\n",
    "\n",
    "        W1 = tf.get_variable(\"W1\", [state_dim, 20],initializer=tf.random_normal_initializer())\n",
    "        b1 = tf.get_variable(\"b1\", [20],initializer=tf.constant_initializer(0))\n",
    "        h1 = tf.nn.tanh(tf.matmul(states, W1) + b1)\n",
    "        W2 = tf.get_variable(\"W2\", [20, 1],initializer=tf.random_normal_initializer())\n",
    "        b2 = tf.get_variable(\"b2\", [1],initializer=tf.constant_initializer(0))\n",
    "        v = tf.matmul(h1, W2) + b2\n",
    "      \n",
    "        return v\n",
    "\n",
    "    return network_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_state_lidar2(env_info):\n",
    "    #Informations d'observation vectorielle\n",
    "    #Dans ce simulateur, la taille de l'observation vectorielle est de 373 .\n",
    "    #0 ~ 359: Données LIDAR (1 particule pour 1 degré)\n",
    "    #360 ~ 362: avertissement gauche, avertissement droit, avertissement avant (0: faux, 1: vrai)\n",
    "    #363: distance avant normalisée\n",
    "    #364: Vitesse du véhicule en marche avant\n",
    "    #365: Vitesse du véhicule hôte\n",
    "    #0 ~ 365 sont utilisés comme données d'entrée pour le capteur\n",
    "    #366 ~ 372 sont utilisés pour envoyer des informations\n",
    "    #366: Nombre de dépassements dans un épisode\n",
    "    #367: Nombre de changement de voie dans un épisode\n",
    "    #368 ~ 372: récompense longitudinale, récompense latérale, \n",
    "    #   récompense de dépassement, récompense de violation, récompense de collision\n",
    "    state=[]\n",
    "    \n",
    "    vector_obs= env_info.vector_observations #recupere les donnees du capteur\n",
    "    \n",
    "    \n",
    "    state.append(vector_obs[0,0]) # 0 degrees \n",
    "    state.append(vector_obs[0,45]) # 45 degrees \n",
    "    state.append(vector_obs[0,90]) # 90 degrees \n",
    "    state.append(vector_obs[0,135]) # 135 degrees \n",
    "    state.append(vector_obs[0,180]) # 180 degrees \n",
    "    state.append(vector_obs[0,225]) # 225 degrees \n",
    "    state.append(vector_obs[0,270]) # 270 degrees \n",
    "    state.append(vector_obs[0,315]) # 315 degrees\n",
    "    \n",
    "    return np.array(state)\n",
    "\n",
    "def get_obs_state_lidar(env_info):\n",
    "    #Informations d'observation vectorielle\n",
    "    #Dans ce simulateur, la taille de l'observation vectorielle est de 373 .\n",
    "    #0 ~ 359: Données LIDAR (1 particule pour 1 degré)\n",
    "    #360 ~ 362: avertissement gauche, avertissement droit, avertissement avant (0: faux, 1: vrai)\n",
    "    #363: distance avant normalisée\n",
    "    #364: Vitesse du véhicule en marche avant\n",
    "    #365: Vitesse du véhicule hôte\n",
    "    #0 ~ 365 sont utilisés comme données d'entrée pour le capteur\n",
    "    #366 ~ 372 sont utilisés pour envoyer des informations\n",
    "    #366: Nombre de dépassements dans un épisode\n",
    "    #367: Nombre de changement de voie dans un épisode\n",
    "    #368 ~ 372: récompense longitudinale, récompense latérale, \n",
    "    #   récompense de dépassement, récompense de violation, récompense de collision\n",
    "    \n",
    "    state = env_info.vector_observations[0][:-7] #recupere les donnees du capteur\n",
    "    \n",
    "    return  np.uint8(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class PolicyGradientActorCritic(object):\n",
    "    \n",
    "    def __init__(self, session,\n",
    "                     optimizer,\n",
    "                     actor_network,\n",
    "                     critic_network,\n",
    "                     state_dim,\n",
    "                     num_actions,\n",
    "                     init_exp=0.1,         # initial exploration prob\n",
    "                     final_exp=0.0,        # final exploration prob\n",
    "                     anneal_steps=500000,    # N steps for annealing exploration\n",
    "                     discount_factor=0.99, # discount future rewards\n",
    "                     reg_param=0.001,      # regularization constants\n",
    "                     max_gradient=5,       # max gradient norms\n",
    "                     summary_writer=None,\n",
    "                     train_mode=True,\n",
    "                     summary_every=1):\n",
    "        \n",
    "        # tensorflow machinery\n",
    "        self.session        = session\n",
    "        self.optimizer      = optimizer\n",
    "        self.summary_writer = summary_writer\n",
    "\n",
    "        # model components\n",
    "        self.actor_network  = actor_network\n",
    "        self.critic_network = critic_network\n",
    "\n",
    "        # training parameters\n",
    "        self.state_dim       = state_dim\n",
    "        self.num_actions     = num_actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.max_gradient    = max_gradient\n",
    "        self.reg_param       = reg_param\n",
    "\n",
    "        # exploration parameters\n",
    "        self.exploration  = init_exp\n",
    "        self.init_exp     = init_exp\n",
    "        self.final_exp    = final_exp\n",
    "        self.anneal_steps = anneal_steps\n",
    "\n",
    "        # counters\n",
    "        self.train_iteration = 0\n",
    "\n",
    "        # rollout buffer\n",
    "        self.state_buffer  = []\n",
    "        self.reward_buffer = []\n",
    "        self.action_buffer = []\n",
    "\n",
    "        self.avg_speed=tf.Variable(0.)\n",
    "        self.avg_overtake=tf.Variable(0.)\n",
    "        self.avg_lanechange=tf.Variable(0.)\n",
    "        self.mean_reward=tf.Variable(0.)\n",
    "        #self.done=False\n",
    "\n",
    "        self.train_mode=train_mode\n",
    "\n",
    "\n",
    "\n",
    "        # create and initialize variables\n",
    "\n",
    "        self.summary_every = summary_every\n",
    "        self.create_variables()\n",
    "        var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        self.session.run(tf.variables_initializer(var_lists))\n",
    "\n",
    "        # make sure all variables are initialized\n",
    "        self.session.run(tf.assert_variables_initialized())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.summary_writer is not None:\n",
    "          # graph was not available when journalist was created\n",
    "          self.summary_writer.add_graph(self.session.graph)\n",
    "          #print(\"self.summary_writer.add_graph(self.session.graph)\")\n",
    "        \n",
    "    def resetModel(self):\n",
    "        self.cleanUp()\n",
    "        self.train_iteration = 0\n",
    "        self.exploration     = self.init_exp\n",
    "        var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        self.session.run(tf.variables_initializer(var_lists))\n",
    "\n",
    "    def create_variables(self):\n",
    "        \n",
    "        with tf.name_scope(\"model_inputs\"):\n",
    "            # raw state representation\n",
    "            self.states = tf.placeholder(tf.float32, (None, self.state_dim), name=\"states\")\n",
    "\n",
    "        # rollout action based on current policy\n",
    "        with tf.name_scope(\"predict_actions\"):\n",
    "            # initialize actor-critic network\n",
    "            with tf.variable_scope(\"actor_network\"):\n",
    "                self.policy_outputs = self.actor_network(self.states)\n",
    "            with tf.variable_scope(\"critic_network\"):\n",
    "                self.value_outputs = self.critic_network(self.states)\n",
    "                \n",
    "            # predict actions from policy network\n",
    "            self.action_scores = tf.identity(self.policy_outputs, name=\"action_scores\")\n",
    "            # Note 1: tf.multinomial is not good enough to use yet\n",
    "            # so we don't use self.predicted_actions for now\n",
    "            self.predicted_actions = tf.multinomial(self.action_scores, 1)\n",
    "\n",
    "        # get variable list\n",
    "        actor_network_variables  = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"actor_network\")\n",
    "        critic_network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"critic_network\")\n",
    "\n",
    "        # compute loss and gradients\n",
    "        with tf.name_scope(\"compute_pg_gradients\"):\n",
    "            # gradients for selecting action from policy network\n",
    "            self.taken_actions = tf.placeholder(tf.int32, (None,), name=\"taken_actions\")\n",
    "            self.discounted_rewards = tf.placeholder(tf.float32, (None,), name=\"discounted_rewards\")\n",
    "\n",
    "            with tf.variable_scope(\"actor_network\", reuse=True):\n",
    "                self.logprobs = self.actor_network(self.states)\n",
    "\n",
    "            with tf.variable_scope(\"critic_network\", reuse=True):\n",
    "                self.estimated_values = self.critic_network(self.states)\n",
    "\n",
    "            # compute policy loss and regularization loss\n",
    "            self.cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logprobs, labels=self.taken_actions)\n",
    "            self.pg_loss            = tf.reduce_mean(self.cross_entropy_loss)\n",
    "            self.actor_reg_loss     = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in actor_network_variables])\n",
    "            self.actor_loss         = self.pg_loss + self.reg_param * self.actor_reg_loss\n",
    "\n",
    "            # compute actor gradients\n",
    "            self.actor_gradients = self.optimizer.compute_gradients(self.actor_loss, actor_network_variables)\n",
    "            # compute advantages A(s) = R - V(s)\n",
    "            self.advantages = tf.reduce_sum(self.discounted_rewards - self.estimated_values)\n",
    "            # compute policy gradients\n",
    "            for i, (grad, var) in enumerate(self.actor_gradients):\n",
    "                if grad is not None:\n",
    "                    self.actor_gradients[i] = (grad * self.advantages, var)\n",
    "\n",
    "            # compute critic gradients\n",
    "            self.mean_square_loss = tf.reduce_mean(tf.square(self.discounted_rewards - self.estimated_values))\n",
    "            self.critic_reg_loss  = tf.reduce_sum([tf.reduce_sum(tf.square(x)) for x in critic_network_variables])\n",
    "            self.critic_loss      = self.mean_square_loss + self.reg_param * self.critic_reg_loss\n",
    "            self.critic_gradients = self.optimizer.compute_gradients(self.critic_loss, critic_network_variables)\n",
    "\n",
    "            # collect all gradients\n",
    "            self.gradients = self.actor_gradients + self.critic_gradients\n",
    "\n",
    "            # clip gradients\n",
    "            for i, (grad, var) in enumerate(self.gradients):\n",
    "                # clip gradients by norm\n",
    "                if grad is not None:\n",
    "                    self.gradients[i] = (tf.clip_by_norm(grad, self.max_gradient), var)\n",
    "\n",
    "            # summarize gradients\n",
    "            #for grad, var in self.gradients:\n",
    "                #tf.summary.histogram(var.name, var)\n",
    "                    #if grad is not None:\n",
    "                         #tf.summary.histogram(var.name + '/gradients', grad)\n",
    "\n",
    "            # emit summaries\n",
    "\n",
    "            tf.summary.scalar('Average_Speed/' + str(self.summary_every) + 'episodes',self.avg_speed)\n",
    "            tf.summary.scalar('Average_overtake/' + str(self.summary_every) + 'episodes',self.avg_overtake)\n",
    "            tf.summary.scalar('Average_lanechange/' + str(self.summary_every) + 'episodes',self.avg_lanechange)\n",
    "            tf.summary.scalar('Average_reward/' + str(self.summary_every) + 'episodes', self.mean_reward)\n",
    "\n",
    "\n",
    "            #tf.summary.histogram(\"estimated_values\", self.estimated_values)\n",
    "            #tf.summary.scalar(\"actor_loss\", self.actor_loss)\n",
    "            #tf.summary.scalar(\"critic_loss\", self.critic_loss)\n",
    "            #tf.summary.scalar(\"reg_loss\", self.actor_reg_loss + self.critic_reg_loss)\n",
    "            \n",
    "        # training update\n",
    "        with tf.name_scope(\"train_actor_critic\"):\n",
    "          # apply gradients to update actor network\n",
    "          self.train_op = self.optimizer.apply_gradients(self.gradients)\n",
    "\n",
    "        self.summarize = tf.summary.merge_all()\n",
    "        self.no_op = tf.no_op()\n",
    "        \n",
    "    def sampleAction(self, states):\n",
    "        # TODO: use this code piece when tf.multinomial gets better\n",
    "        # sample action from current policy\n",
    "        # actions = self.session.run(self.predicted_actions, {self.states: states})[0]\n",
    "        # return actions[0]\n",
    "\n",
    "        # temporary workaround\n",
    "        def softmax(y):\n",
    "            \"\"\" simple helper function here that takes unnormalized logprobs \"\"\"\n",
    "            maxy = np.amax(y)\n",
    "            e = np.exp(y - maxy)\n",
    "            return e / np.sum(e)\n",
    "\n",
    "        # epsilon-greedy exploration strategy\n",
    "        if self.train_mode:\n",
    "            if random.random() < self.exploration and self.train_mode:\n",
    "                return random.randint(0, self.num_actions-1)\n",
    "            else:\n",
    "                action_scores = self.session.run(self.action_scores, {self.states: states})[0]\n",
    "                action_probs  = softmax(action_scores) #- 1e-5\n",
    "                action_probs=np.asarray(action_probs).astype('float64')\n",
    "                action_probs= action_probs/np.sum(action_probs)\n",
    "                #print(\"action_scores\",action_scores)\n",
    "                #print(\"softmax(action_scores)\",softmax(action_scores))\n",
    "                #print(\"action_probs\",action_probs)\n",
    "                #print(\"np.random.multinomial(1, action_probs)\",np.random.multinomial(1, action_probs))\n",
    "                action = np.argmax(np.random.multinomial(1, action_probs))\n",
    "                return action\n",
    "        else:\n",
    "            action_scores = self.session.run(self.action_scores, {self.states: states})[0]\n",
    "            action_probs  = softmax(action_scores) #- 1e-5\n",
    "            action_probs=np.asarray(action_probs).astype('float64')\n",
    "            action_probs= action_probs/np.sum(action_probs)\n",
    "            action = np.argmax(np.random.multinomial(1, action_probs))\n",
    "            return action\n",
    "\n",
    "    def updateModel(self,step):\n",
    "        N = len(self.reward_buffer)\n",
    "        r = 0 # use discounted reward to approximate Q value\n",
    "\n",
    "        # compute discounted future rewards\n",
    "        discounted_rewards = np.zeros(N)\n",
    "        for t in reversed(range(N)):\n",
    "            # future discounted reward from now on\n",
    "            r = self.reward_buffer[t] + self.discount_factor * r\n",
    "            discounted_rewards[t] = r\n",
    "        # whether to calculate summaries\n",
    "        calculate_summaries = self.summary_writer is not None and self.train_iteration % self.summary_every == 0\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"self.train_iteration\",self.train_iteration)\n",
    "        #print(\"self.summary_every\",self.summary_every)\n",
    "        #print(\"calculate_summaries\",calculate_summaries)\n",
    "\n",
    "        # update policy network with the rollout in batches\n",
    "        if self.train_mode:\n",
    "            summary_str = self.session.run(self.summarize if calculate_summaries else self.no_op)\n",
    "             # emit summaries\n",
    "            if calculate_summaries:\n",
    "                #print(\"summary_writer\")\n",
    "                self.summary_writer.add_summary(summary_str,step)\n",
    "                \n",
    "            for t in range(N-1):\n",
    "                # prepare inputs\n",
    "                states  = self.state_buffer[t][np.newaxis, :]\n",
    "                actions = np.array([self.action_buffer[t]])\n",
    "                rewards = np.array([discounted_rewards[t]])\n",
    "\n",
    "                # perform one update of training\n",
    "                res= self.session.run(self.train_op if self.train_mode else self.no_op,{\n",
    "                  self.states:             states,\n",
    "                  self.taken_actions:      actions,\n",
    "                  self.discounted_rewards: rewards\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            summary_str = self.session.run(self.summarize if calculate_summaries else self.no_op)\n",
    "            if calculate_summaries:\n",
    "                #print(\"summary_writer\")\n",
    "                self.summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "\n",
    "\n",
    "        self.annealExploration()\n",
    "        self.train_iteration += 1\n",
    "\n",
    "        # clean up\n",
    "        self.cleanUp()\n",
    "\n",
    "    def annealExploration(self, stategy='linear'):\n",
    "        ratio = max((self.anneal_steps - self.train_iteration)/float(self.anneal_steps), 0)\n",
    "        self.exploration = (self.init_exp - self.final_exp) * ratio + self.final_exp\n",
    "\n",
    "    def storeRollout(self, state, action, reward):\n",
    "        self.action_buffer.append(action)\n",
    "        #print(\"self.action_buffer\",self.action_buffer)\n",
    "        self.reward_buffer.append(reward)\n",
    "        self.state_buffer.append(state)\n",
    "\n",
    "    def cleanUp(self):\n",
    "        self.state_buffer  = []\n",
    "        self.reward_buffer = []\n",
    "        self.action_buffer = []\n",
    "\n",
    "\n",
    "      # ================================================================\n",
    "      # Saving variables\n",
    "      # ================================================================\n",
    "\n",
    "    def load_state(self,fname):\n",
    "        #import logger\n",
    "        #logger.warn('load_state method is deprecated, please use load_variables instead')\n",
    "        #sess = sess or get_session()\n",
    "        #saver = tf.train.Saver()\n",
    "        #First let's load meta graph and restore weights\n",
    "        #saver = tf.train.import_meta_graph('saved_networks/model.meta')\n",
    "        #saver.restore(sess,tf.train.latest_checkpoint('saved_networks/'))\n",
    "        #saver.restore(tf.get_default_session(), fname)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.session, fname)\n",
    "\n",
    "    def save_state(self,fname):\n",
    "        #import logger\n",
    "        #logger.warn('save_state method is deprecated, please use save_variables instead')\n",
    "        #sess = sess or get_session()\n",
    "        dirname = os.path.dirname(fname)\n",
    "        if any(dirname):\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.session, fname)\n",
    "\n",
    "    # The methods above and below are clearly doing the same thing, and in a rather similar way\n",
    "    # TODO: ensure there is no subtle differences and remove one\n",
    "\n",
    "    def save_variables(self,save_path, variables=None, sess=None):\n",
    "        import joblib\n",
    "        sess = sess or self.session\n",
    "        variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "        ps = sess.run(variables)\n",
    "        save_dict = {v.name: value for v, value in zip(variables, ps)}\n",
    "        dirname = os.path.dirname(save_path)\n",
    "        if any(dirname):\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "        joblib.dump(save_dict, save_path)\n",
    "\n",
    "    def load_variables(self,load_path, variables=None, sess=None):\n",
    "        import joblib\n",
    "        sess = sess or self.session\n",
    "        variables = variables or tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "        loaded_params = joblib.load(os.path.expanduser(load_path))\n",
    "        restores = []\n",
    "        if isinstance(loaded_params, list):\n",
    "            assert len(loaded_params) == len(variables), 'number of variables loaded mismatches len(variables)'\n",
    "            for d, v in zip(loaded_params, variables):\n",
    "                restores.append(v.assign(d))\n",
    "        else:\n",
    "            for v in variables:\n",
    "                restores.append(v.assign(loaded_params[v.name]))\n",
    "\n",
    "        sess.run(restores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learn(env,network='mlp',\n",
    "          seed=None,\n",
    "          lr=5e-4,\n",
    "          total_timesteps=2000000,\n",
    "          buffer_size=100000,\n",
    "          exploration_fraction=0.1,\n",
    "          exploration_final_eps=0.02,\n",
    "          train_freq=1000,\n",
    "          batch_size=500,\n",
    "          print_freq=2,\n",
    "          checkpoint_freq=10000,\n",
    "          checkpoint_path=None,\n",
    "          learning_starts=50000,\n",
    "          gamma=0.99,\n",
    "          load_path=None,\n",
    "          train_mode = True,\n",
    "         ):\n",
    "    \"\"\"Train a deepq model.\n",
    "    Parameters\n",
    "    -------\n",
    "    env: gym.Env\n",
    "        environment to train on\n",
    "    network: string or a function\n",
    "        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n",
    "        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n",
    "        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n",
    "    seed: int or None\n",
    "        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n",
    "    lr: float\n",
    "        learning rate for adam optimizer\n",
    "    total_timesteps: int\n",
    "        number of env steps to optimizer for\n",
    "    buffer_size: int\n",
    "        size of the replay buffer\n",
    "    exploration_fraction: float\n",
    "        fraction of entire training period over which the exploration rate is annealed\n",
    "    exploration_final_eps: float\n",
    "        final value of random action probability\n",
    "    train_freq: int\n",
    "        update the model every `train_freq` steps.\n",
    "    batch_size: int\n",
    "        size of a batch sampled from replay buffer for training\n",
    "    print_freq: int\n",
    "        how often to print out training progress\n",
    "        set to None to disable printing\n",
    "    checkpoint_freq: int\n",
    "        how often to save the model. This is so that the best version is restored\n",
    "        at the end of the training. If you do not wish to restore the best version at\n",
    "        the end of the training set this variable to None.\n",
    "    learning_starts: int\n",
    "        how many steps of the model to collect transitions for before learning starts\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    target_network_update_freq: int\n",
    "        update the target network every `target_network_update_freq` steps.\n",
    "    prioritized_replay: True\n",
    "        if True prioritized replay buffer will be used.\n",
    "    prioritized_replay_alpha: float\n",
    "        alpha parameter for prioritized replay buffer\n",
    "    prioritized_replay_beta0: float\n",
    "        initial value of beta for prioritized replay buffer\n",
    "    prioritized_replay_beta_iters: int\n",
    "        number of iterations over which beta will be annealed from initial value\n",
    "        to 1.0. If set to None equals to total_timesteps.\n",
    "    prioritized_replay_eps: float\n",
    "        epsilon to add to the TD errors when updating priorities.\n",
    "    param_noise: bool\n",
    "        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n",
    "    callback: (locals, globals) -> None\n",
    "        function called at every steps with state of the algorithm.\n",
    "        If callback returns true training stops.\n",
    "    load_path: str\n",
    "        path to load the model from. (default: None)\n",
    "    **network_kwargs\n",
    "        additional keyword arguments to pass to the network builder.\n",
    "    Returns\n",
    "    -------\n",
    "    act: ActWrapper\n",
    "        Wrapper over act function. Adds ability to save it and load it.\n",
    "        See header of baselines/deepq/categorical.py for details on the act function.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Create all the functions necessary to train the model\n",
    "    \n",
    "    \n",
    "    GPU_fraction = 0.4\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = GPU_fraction\n",
    "\n",
    "    #sess = get_session()\n",
    "    #set_global_seeds(seed)\n",
    "    \n",
    "    sess = tf.Session(config=config)\n",
    "    #sess = tf.Session()\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.0001, decay=0.9)\n",
    "    \n",
    "    \n",
    "    # Examine environment parameters\n",
    "    #print(str(env))\n",
    "    # Set the default brain to work with\n",
    "    default_brain = env.brain_names[0]\n",
    "    brain = env.brains[default_brain]\n",
    "\n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    reset = True\n",
    "    state = get_obs_state_lidar(env_info)\n",
    "    \n",
    "    num_actions=brain.vector_action_space_size[0]\n",
    "    state_dim   = state.shape[0]\n",
    "    #num_actions = env.action_space.n\n",
    "\n",
    "    #observation_space=obs.copy()\n",
    "    \n",
    "    #start_tensorboard\n",
    "    #th = threading.Thread(target=start_tensorboard, args=([sess]))\n",
    "    #th.start()\n",
    "    \n",
    "   \n",
    "    \n",
    "     # date - hour - minute of training time\n",
    "    date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "     # Make folder for save data\n",
    "    os.makedirs(checkpoint_path + date_time + '_ACTOR_CRTIC_sensor')\n",
    "\n",
    "    # Summary for tensorboard\n",
    "    writer = tf.summary.FileWriter(checkpoint_path + date_time + '_ACTOR_CRTIC_sensor')\n",
    "    #summary_placeholders, update_ops, summary_op = setup_summary(print_freq)\n",
    "    #summary_writer = tf.summary.FileWriter(checkpoint_path + date_time + '_DQN_sensor', sess.graph)\n",
    "    \n",
    "    \n",
    "    actor_network=build_actor_network(state_dim,num_actions)\n",
    "    critic_network=build_critic_network(state_dim,num_actions)\n",
    "    \n",
    "    pg_reinforce = PolicyGradientActorCritic(sess,optimizer,actor_network,critic_network,state_dim,num_actions,\n",
    "                                             summary_writer=writer,summary_every=print_freq,train_mode=train_mode)\n",
    "    \n",
    "    \n",
    "    summary_vars = [pg_reinforce.avg_speed,pg_reinforce.avg_overtake, pg_reinforce.avg_lanechange,\n",
    "                                pg_reinforce.mean_reward]\n",
    "    \n",
    "    summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "    update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    \n",
    "\n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    #initialize()\n",
    "    #update_target()\n",
    "\n",
    "    episode_rewards = [0.0]\n",
    "    saved_mean_reward = None\n",
    "    #obs = env.reset()\n",
    "    speed_list = []\n",
    "    overtake_list = []\n",
    "    lanechange_list = []\n",
    "    \n",
    "    no_reward_since = 0\n",
    "    episode_history = deque(maxlen=100)\n",
    "    \n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        td = checkpoint_path + date_time + '_ACTOR_CRTIC_sensor' or td\n",
    "\n",
    "        model_file = os.path.join(td, \"model\")\n",
    "        model_saved = False\n",
    "        \n",
    "\n",
    "        if tf.train.latest_checkpoint(td) is not None:\n",
    "            pg_reinforce.load_variables(model_file)\n",
    "            #logger.log('Loaded model from {}'.format(model_file))\n",
    "            print('Loaded model from {}'.format(model_file))\n",
    "            model_saved = True\n",
    "        elif load_path is not None:\n",
    "            pg_reinforce.load_variables(load_path)\n",
    "            #logger.log('Loaded model from {}'.format(load_path))\n",
    "            print('Loaded model from {}'.format(load_path))\n",
    "            \n",
    "            \n",
    "        pg_reinforce.updateModel(0)\n",
    "        for t in range(total_timesteps):\n",
    "                \n",
    "            # Take action and update exploration to the newest value\n",
    "            action = pg_reinforce.sampleAction(state[np.newaxis,:])\n",
    "            env_action = action\n",
    "            reset = False\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "             # Get information for plotting\n",
    "\n",
    "            vehicle_speed  = 100 * env_info.vector_observations[0][-8]\n",
    "            num_overtake   = env_info.vector_observations[0][-7]\n",
    "            num_lanechange = env_info.vector_observations[0][-6]\n",
    "\n",
    "\n",
    "            # Get information for update\n",
    "            env_info = env.step(action)[default_brain]\n",
    "            next_state = get_obs_state_lidar(env_info)\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "\n",
    "            #total_rewards += reward\n",
    "            #reward = 5.0 if done else -0.1\n",
    "\n",
    "            pg_reinforce.storeRollout(state,action,reward)\n",
    "            state=next_state\n",
    "            \n",
    "            \n",
    "            episode_rewards[-1] += reward\n",
    "            num_episodes = len(episode_rewards)\n",
    "            \n",
    "            tab=episode_rewards[-print_freq-1:-1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            if len(tab)==0:\n",
    "                mean_reward=episode_rewards[-1]\n",
    "            else:\n",
    "                mean_reward = round(np.mean(tab), 1) #mean reward of last print_freq episode\n",
    "            \n",
    "            speed_list.append(vehicle_speed)\n",
    "            overtake_list.append(num_overtake)\n",
    "            lanechange_list.append(num_lanechange)\n",
    "            \n",
    "\n",
    "            if done:\n",
    "                #eps = tf.get_variable(\"eps\", ())\n",
    "                #Epsilon=eps.eval()\n",
    "\n",
    "                # Print informations if terminal\n",
    "                print('step: ' + str(t) + ' / '  + 'episode: ' + str(num_episodes) + ' / ' + ' episode_rewards: ' + str(episode_rewards[-1]))\n",
    "\n",
    "                env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "                reset = True\n",
    "                state = get_obs_state_lidar(env_info)\n",
    "                #obs = env.reset()\n",
    "                \n",
    "                #reset = True\n",
    "\n",
    "                avg_speed = np.mean(speed_list)\n",
    "                avg_overtake = np.mean(overtake_list)\n",
    "                avg_lanechange = np.mean(lanechange_list)\n",
    "                \n",
    "                \n",
    "                \n",
    "                tensorboard_info = [avg_speed, avg_overtake, avg_lanechange,mean_reward]\n",
    "                \n",
    "                for i in range(len(tensorboard_info)):\n",
    "                    sess.run(update_ops[i], feed_dict = {summary_placeholders[i]: float(tensorboard_info[i])})\n",
    "                    \n",
    "                # if we don't see rewards in consecutive episodes\n",
    "                # it's likely that the model gets stuck in bad local optima\n",
    "                # we simply reset the model and try again\n",
    "\n",
    "                if episode_rewards[-1] <= -300:\n",
    "                    no_reward_since += 1\n",
    "                    if no_reward_since >= 5:\n",
    "                        # create and initialize variables\n",
    "                        print('Resetting model... start anew!')\n",
    "                        pg_reinforce.resetModel()\n",
    "                        no_reward_since = 0\n",
    "                        continue\n",
    "                    else:\n",
    "                        no_reward_since = 0\n",
    "                        \n",
    "                episode_rewards.append(0.0)\n",
    "                \n",
    "                pg_reinforce.updateModel(t)\n",
    "                \n",
    "                speed_list = []\n",
    "                overtake_list = []\n",
    "                lanechange_list = []\n",
    "                \n",
    "            \n",
    "                \n",
    "            if (train_mode and checkpoint_freq is not None and num_episodes >= print_freq and t % checkpoint_freq == 0):\n",
    "\n",
    "                if saved_mean_reward is None or mean_reward > saved_mean_reward:\n",
    "\n",
    "                    if print_freq is not None:\n",
    "                        #logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
    "                                   #saved_mean_reward, mean_reward))\n",
    "\n",
    "                        print(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
    "                                   saved_mean_reward, mean_reward))\n",
    "                    pg_reinforce.save_variables(model_file)\n",
    "                    model_saved = True\n",
    "                    saved_mean_reward = mean_reward\n",
    "                    \n",
    "        if model_saved:\n",
    "            if print_freq is not None:\n",
    "                #logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
    "                print(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
    "                pg_reinforce.load_variables(model_file)\n",
    "\n",
    "                    \n",
    "             \n",
    "\n",
    "    return pg_reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pg_reinforce=learn(env,checkpoint_path='saved_networks/',train_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.multinomial(1, [1.0 / 3, 2.0 / 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=[0.31795933,0.16079315,0.21846536,0.15712377,0.14565839]\n",
    "#a = np.asarray(a).astype('float32')\n",
    "#a = a/np.sum (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.multinomial(1, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.astype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
