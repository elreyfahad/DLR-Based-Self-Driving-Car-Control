{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from dqn import logger\n",
    "from dqn.commons.schedules import LinearSchedule\n",
    "from mlagents.envs import UnityEnvironment\n",
    "from dqn.tf_util import get_session,initialize\n",
    "from dqn.models import build_q_func\n",
    "from dqn.utils import adjust_shape\n",
    "from dqn.build_graph import build_train\n",
    "from dqn.utils import ObservationInput\n",
    "from dqn.replay_buffer import ReplayBuffer,PrioritizedReplayBuffer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment path\n",
    "\n",
    "Be sure to set `env_name` to the name of the Unity environment file you want to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_ = \"Windows\"\n",
    "\n",
    "env_name = \"../environment/\" + os_ + \"/Driving\" # Name of the Unity environment binary to launch\n",
    "train_mode = True # Whether to run the environment in training or inference mode\n",
    "#help(UnityEnvironment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the environment\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_name,worker_id=4, base_port=5005)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the observation and state spaces\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, _states_ refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, _observations_ refer to a set of relevant pixel-wise visuals for an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Sensor data (LIDAR): \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "Num_obs = len(env_info.visual_observations)\n",
    "\n",
    "print(\"Image data (Front Camera): \\n{}\")\n",
    "if Num_obs > 1:\n",
    "    f, axarr = plt.subplots(1, Num_obs, figsize=(20,10))\n",
    "    for i, observation in enumerate(env_info.visual_observations):\n",
    "        if observation.shape[3] == 3:\n",
    "            axarr[i].imshow(observation[0,:,:,:])\n",
    "            axarr[i].axis('off')\n",
    "        else:\n",
    "            axarr[i].imshow(observation[0,:,:,0])\n",
    "            axarr[i].axis('off')\n",
    "else:\n",
    "    f, axarr = plt.subplots(1, Num_obs)\n",
    "    for i, observation in enumerate(env_info.visual_observations):\n",
    "        if observation.shape[3] == 3:\n",
    "            axarr.imshow(observation[0,:,:,:])\n",
    "            axarr.axis('off')\n",
    "        else:\n",
    "            axarr.imshow(observation[0,:,:,0])\n",
    "            axarr.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "import cloudpickle\n",
    "\n",
    "class ActWrapper(object):\n",
    "    def __init__(self, act, act_params):\n",
    "        self._act = act\n",
    "        self._act_params = act_params\n",
    "        self.initial_state = None\n",
    "\n",
    "    @staticmethod\n",
    "    def load_act(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            model_data, act_params = cloudpickle.load(f)\n",
    "        act = build_act(**act_params)\n",
    "        sess = tf.Session()\n",
    "        sess.__enter__()\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            arc_path = os.path.join(td, \"packed.zip\")\n",
    "            with open(arc_path, \"wb\") as f:\n",
    "                f.write(model_data)\n",
    "\n",
    "            zipfile.ZipFile(arc_path, 'r', zipfile.ZIP_DEFLATED).extractall(td)\n",
    "            load_variables(os.path.join(td, \"model\"))\n",
    "\n",
    "        return ActWrapper(act, act_params)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self._act(*args, **kwargs)\n",
    "\n",
    "    def step(self, observation, **kwargs):\n",
    "        # DQN doesn't use RNNs so we ignore states and masks\n",
    "        kwargs.pop('S', None)\n",
    "        kwargs.pop('M', None)\n",
    "        return self._act([observation], **kwargs), None, None, None\n",
    "\n",
    "    def save_act(self, path=None):\n",
    "        \"\"\"Save model to a pickle located at `path`\"\"\"\n",
    "        if path is None:\n",
    "            path = os.path.join(logger.get_dir(), \"model.pkl\")\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            save_variables(os.path.join(td, \"model\"))\n",
    "            arc_name = os.path.join(td, \"packed.zip\")\n",
    "            with zipfile.ZipFile(arc_name, 'w') as zipf:\n",
    "                for root, dirs, files in os.walk(td):\n",
    "                    for fname in files:\n",
    "                        file_path = os.path.join(root, fname)\n",
    "                        if file_path != arc_name:\n",
    "                            zipf.write(file_path, os.path.relpath(file_path, td))\n",
    "            with open(arc_name, \"rb\") as f:\n",
    "                model_data = f.read()\n",
    "        with open(path, \"wb\") as f:\n",
    "            cloudpickle.dump((model_data, self._act_params), f)\n",
    "\n",
    "    def save(self, path):\n",
    "        save_variables(path)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_state_lidar(env_info):\n",
    "    #Informations d'observation vectorielle\n",
    "    #Dans ce simulateur, la taille de l'observation vectorielle est de 373 .\n",
    "    #0 ~ 359: Données LIDAR (1 particule pour 1 degré)\n",
    "    #360 ~ 362: avertissement gauche, avertissement droit, avertissement avant (0: faux, 1: vrai)\n",
    "    #363: distance avant normalisée\n",
    "    #364: Vitesse du véhicule en marche avant\n",
    "    #365: Vitesse du véhicule hôte\n",
    "    #0 ~ 365 sont utilisés comme données d'entrée pour le capteur\n",
    "    #366 ~ 372 sont utilisés pour envoyer des informations\n",
    "    #366: Nombre de dépassements dans un épisode\n",
    "    #367: Nombre de changement de voie dans un épisode\n",
    "    #368 ~ 372: récompense longitudinale, récompense latérale, \n",
    "    #   récompense de dépassement, récompense de violation, récompense de collision\n",
    "    \n",
    "    state = env_info.vector_observations[0][:-7] #recupere les donnees du capteur\n",
    "    \n",
    "    return  np.uint8(state)\n",
    "\n",
    "def state_initialization(env_info,Num_stackFram,Num_skipFrame,Num_dataSize): \n",
    "    #Informations d'observation vectorielle\n",
    "    #Dans ce simulateur, la taille de l'observation vectorielle est de 373 .\n",
    "    #0 ~ 359: Données LIDAR (1 particule pour 1 degré)\n",
    "    #360 ~ 362: avertissement gauche, avertissement droit, avertissement avant (0: faux, 1: vrai)\n",
    "    #363: distance avant normalisée\n",
    "    #364: Vitesse du véhicule en marche avant\n",
    "    #365: Vitesse du véhicule hôte\n",
    "    #0 ~ 365 sont utilisés comme données d'entrée pour le capteur\n",
    "    #366 ~ 372 sont utilisés pour envoyer des informations\n",
    "    #366: Nombre de dépassements dans un épisode\n",
    "    #367: Nombre de changement de voie dans un épisode\n",
    "    #368 ~ 372: récompense longitudinale, récompense latérale, \n",
    "    #   récompense de dépassement, récompense de violation, récompense de collision\n",
    "    \n",
    "\n",
    "    \n",
    "    state = env_info.vector_observations[0][:-7] #recupere les donnees du capteur\n",
    "\n",
    "    state_set = []\n",
    "\n",
    "    for i in range(Num_skipFrame * Num_stackFrame):\n",
    "        state_set.append(state)\n",
    "    \n",
    "    # Stack the frame according to the number of skipping and stacking frames using observation set\n",
    "    state_stack = np.zeros((Num_stackFrame, Num_dataSize))\n",
    "    for stack_frame in range(Num_stackFrame):\n",
    "        state_stack[(Num_stackFrame - 1) - stack_frame, :] = state_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "\n",
    "    state_stack = np.uint8(state_stack)\n",
    "\n",
    "    return state_stack, state_set\n",
    "\n",
    "# Resize input information \n",
    "def resize_state(env_info, state_set,Num_stackFram,Num_skipFrame,Num_dataSize):\n",
    "    state = env_info.vector_observations[0][:-7]\n",
    "\n",
    "    # Add state to the state_set\n",
    "    state_set.append(state)\n",
    "    \n",
    "    # Stack the frame according to the number of skipping and stacking frames using observation set\n",
    "    state_stack = np.zeros((Num_stackFrame, Num_dataSize))\n",
    "    for stack_frame in range(Num_stackFrame):\n",
    "        state_stack[(Num_stackFrame - 1) - stack_frame, :] = state_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "        \n",
    "    del state_set[0]\n",
    "\n",
    "    state_stack = np.uint8(state_stack)\n",
    "    \n",
    "    return state_stack, state_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for tensorboard\n",
    "def setup_summary(Num_plot_episode):\n",
    "    \n",
    "    episode_speed      = tf.Variable(0.)\n",
    "    episode_overtake   = tf.Variable(0.)\n",
    "    episode_lanechange = tf.Variable(0.)\n",
    "    episode_rewards    =tf.Variable(0.)\n",
    "\n",
    "    tf.summary.scalar('Average_Speed/' + str(Num_plot_episode) + 'episodes', episode_speed)\n",
    "    tf.summary.scalar('Average_overtake/' + str(Num_plot_episode) + 'episodes', episode_overtake)\n",
    "    tf.summary.scalar('Average_lanechange/' + str(Num_plot_episode) + 'episodes', episode_lanechange)\n",
    "    \n",
    "    tf.summary.scalar('Average_reward/' + str(Num_plot_episode) + 'episodes', episode_rewards)\n",
    "\n",
    "\n",
    "    summary_vars = [episode_speed, episode_overtake, episode_lanechange,episode_rewards]\n",
    "    summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "    update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Tensorboard interfacing\n",
    "# ================================================================\n",
    "    \n",
    "import os.path as osp \n",
    "import threading, time\n",
    "\n",
    "def launch_tensorboard_in_background(log_dir):\n",
    "\n",
    "    import subprocess\n",
    "    subprocess.Popen(['tensorboard', '--logdir', log_dir])\n",
    "\n",
    "def start_tensorboard(session):\n",
    "    \n",
    "    time.sleep(10) # Wait until graph is setup\n",
    "    tb_path = osp.join(logger.get_dir(), 'tb')\n",
    "    summary_writer = tf.summary.FileWriter(tb_path, graph=session.graph)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    launch_tensorboard_in_background(tb_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_act(path):\n",
    "    \"\"\"Load act function that was returned by learn function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        path to the act function pickle\n",
    "    Returns\n",
    "    -------\n",
    "    act: ActWrapper\n",
    "        function that takes a batch of observations\n",
    "        and returns actions.\n",
    "    \"\"\"\n",
    "    return ActWrapper.load_act(path)\n",
    "\n",
    "\n",
    "def learn(env,network='mlp',\n",
    "          seed=None,\n",
    "          lr=5e-4,\n",
    "          total_timesteps=10000000,\n",
    "          buffer_size=100000,\n",
    "          exploration_fraction=0.1,\n",
    "          exploration_final_eps=0.02,\n",
    "          train_freq=1000,\n",
    "          batch_size=500,\n",
    "          print_freq=2,\n",
    "          checkpoint_freq=10000,\n",
    "          checkpoint_path=None,\n",
    "          learning_starts=50000,\n",
    "          gamma=0.99,\n",
    "          target_network_update_freq=10000,\n",
    "          prioritized_replay=True,\n",
    "          prioritized_replay_alpha=0.6,\n",
    "          prioritized_replay_beta0=0.4,\n",
    "          prioritized_replay_beta_iters=None,\n",
    "          prioritized_replay_eps=1e-6,\n",
    "          param_noise=True,\n",
    "          callback=None,\n",
    "          load_path=None,\n",
    "          train_mode = True,\n",
    "          **network_kwargs\n",
    "         ):\n",
    "    \"\"\"Train a deepq model.\n",
    "    Parameters\n",
    "    -------\n",
    "    env: gym.Env\n",
    "        environment to train on\n",
    "    network: string or a function\n",
    "        neural network to use as a q function approximator. If string, has to be one of the names of registered models in baselines.common.models\n",
    "        (mlp, cnn, conv_only). If a function, should take an observation tensor and return a latent variable tensor, which\n",
    "        will be mapped to the Q function heads (see build_q_func in baselines.deepq.models for details on that)\n",
    "    seed: int or None\n",
    "        prng seed. The runs with the same seed \"should\" give the same results. If None, no seeding is used.\n",
    "    lr: float\n",
    "        learning rate for adam optimizer\n",
    "    total_timesteps: int\n",
    "        number of env steps to optimizer for\n",
    "    buffer_size: int\n",
    "        size of the replay buffer\n",
    "    exploration_fraction: float\n",
    "        fraction of entire training period over which the exploration rate is annealed\n",
    "    exploration_final_eps: float\n",
    "        final value of random action probability\n",
    "    train_freq: int\n",
    "        update the model every `train_freq` steps.\n",
    "    batch_size: int\n",
    "        size of a batch sampled from replay buffer for training\n",
    "    print_freq: int\n",
    "        how often to print out training progress\n",
    "        set to None to disable printing\n",
    "    checkpoint_freq: int\n",
    "        how often to save the model. This is so that the best version is restored\n",
    "        at the end of the training. If you do not wish to restore the best version at\n",
    "        the end of the training set this variable to None.\n",
    "    learning_starts: int\n",
    "        how many steps of the model to collect transitions for before learning starts\n",
    "    gamma: float\n",
    "        discount factor\n",
    "    target_network_update_freq: int\n",
    "        update the target network every `target_network_update_freq` steps.\n",
    "    prioritized_replay: True\n",
    "        if True prioritized replay buffer will be used.\n",
    "    prioritized_replay_alpha: float\n",
    "        alpha parameter for prioritized replay buffer\n",
    "    prioritized_replay_beta0: float\n",
    "        initial value of beta for prioritized replay buffer\n",
    "    prioritized_replay_beta_iters: int\n",
    "        number of iterations over which beta will be annealed from initial value\n",
    "        to 1.0. If set to None equals to total_timesteps.\n",
    "    prioritized_replay_eps: float\n",
    "        epsilon to add to the TD errors when updating priorities.\n",
    "    param_noise: bool\n",
    "        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)\n",
    "    callback: (locals, globals) -> None\n",
    "        function called at every steps with state of the algorithm.\n",
    "        If callback returns true training stops.\n",
    "    load_path: str\n",
    "        path to load the model from. (default: None)\n",
    "    **network_kwargs\n",
    "        additional keyword arguments to pass to the network builder.\n",
    "    Returns\n",
    "    -------\n",
    "    act: ActWrapper\n",
    "        Wrapper over act function. Adds ability to save it and load it.\n",
    "        See header of baselines/deepq/categorical.py for details on the act function.\n",
    "\n",
    "    \"\"\"\n",
    "    # Examine environment parameters\n",
    "    #print(str(env))\n",
    "    # Set the default brain to work with\n",
    "    default_brain = env.brain_names[0]\n",
    "    brain = env.brains[default_brain]\n",
    "\n",
    "    num_actions=brain.vector_action_space_size[0]\n",
    "    \n",
    "    # Create all the functions necessary to train the model\n",
    "    \n",
    "    \n",
    "    GPU_fraction = 0.4\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = GPU_fraction\n",
    "\n",
    "    sess = get_session(config=config)\n",
    "    #set_global_seeds(seed)\n",
    "\n",
    "    q_func = build_q_func(network, **network_kwargs)\n",
    "\n",
    "    # capture the shape outside the closure so that the env object is not serialized\n",
    "    # by cloudpickle when serializing make_obs_ph\n",
    "\n",
    "    #observation_space = env.observation_space\n",
    "\n",
    "    \n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    reset = True\n",
    "    obs = get_obs_state_lidar(env_info)\n",
    "\n",
    "    observation_space=obs.copy()\n",
    "    \n",
    "    #start_tensorboard\n",
    "    #th = threading.Thread(target=start_tensorboard, args=([sess]))\n",
    "    #th.start()\n",
    "    \n",
    "   \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #def make_obs_ph(name,Num_action):\n",
    "\n",
    "    #    tf.placeholder(shape=(None,) + state.shape, dtype=state.dtype, name='st')\n",
    "        \n",
    "    #    return tf.placeholder(tf.float32, shape = [None, Num_action],name=name)\n",
    "    \n",
    "\n",
    "    def make_obs_ph(name):\n",
    "        return ObservationInput(observation_space, name=name)\n",
    "\n",
    "    act, train, update_target, debug =build_train(\n",
    "        make_obs_ph=make_obs_ph,\n",
    "        q_func=q_func,\n",
    "        num_actions=num_actions,\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate=lr),\n",
    "        gamma=gamma,\n",
    "        grad_norm_clipping=10,\n",
    "        param_noise=param_noise\n",
    "    )\n",
    "\n",
    "    act_params = {\n",
    "        'make_obs_ph': make_obs_ph,\n",
    "        'q_func': q_func,\n",
    "        'num_actions': num_actions,\n",
    "    }\n",
    "\n",
    "    act = ActWrapper(act, act_params)\n",
    "\n",
    "    # Create the replay buffer\n",
    "    if prioritized_replay:\n",
    "        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)\n",
    "        if prioritized_replay_beta_iters is None:\n",
    "            prioritized_replay_beta_iters = total_timesteps\n",
    "            \n",
    "        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,\n",
    "                                       initial_p=prioritized_replay_beta0,\n",
    "                                       final_p=1.0)\n",
    "    else:\n",
    "        replay_buffer = ReplayBuffer(buffer_size)\n",
    "        beta_schedule = None\n",
    "        \n",
    "    # Create the schedule for exploration starting from 1.\n",
    "    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * total_timesteps),\n",
    "                                 initial_p=1.0,\n",
    "                                 final_p=exploration_final_eps)\n",
    "    \n",
    "     # date - hour - minute of training time\n",
    "    date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "     # Make folder for save data\n",
    "    os.makedirs(checkpoint_path + date_time + '_DQN_sensor')\n",
    "    speed_list = []\n",
    "    overtake_list = []\n",
    "    lanechange_list = []\n",
    "\n",
    "    # Summary for tensorboard\n",
    "    summary_placeholders, update_ops, summary_op = setup_summary(print_freq)\n",
    "    summary_writer = tf.summary.FileWriter(checkpoint_path + date_time + '_DQN_sensor', sess.graph)\n",
    "    \n",
    "\n",
    "    # Initialize the parameters and copy them to the target network.\n",
    "    initialize()\n",
    "    update_target()\n",
    "\n",
    "    episode_rewards = [0.0]\n",
    "    saved_mean_reward = None\n",
    "    #obs = env.reset()\n",
    "    speed_list = []\n",
    "    overtake_list = []\n",
    "    lanechange_list = []\n",
    "    \n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        td = checkpoint_path + date_time + '_DQN_sensor' or td\n",
    "\n",
    "        model_file = os.path.join(td, \"model\")\n",
    "        model_saved = False\n",
    "        \n",
    "\n",
    "        if tf.train.latest_checkpoint(td) is not None:\n",
    "            load_state(model_file)\n",
    "            logger.log('Loaded model from {}'.format(model_file))\n",
    "            print('Loaded model from {}'.format(model_file))\n",
    "            model_saved = True\n",
    "        elif load_path is not None:\n",
    "            load_state(load_path)\n",
    "            logger.log('Loaded model from {}'.format(load_path))\n",
    "            print('Loaded model from {}'.format(load_path))\n",
    "            \n",
    "            \n",
    "            \n",
    "       \n",
    "        \n",
    "        \n",
    "        for t in range(total_timesteps):\n",
    "            if callback is not None:\n",
    "                if callback(locals(), globals()):\n",
    "                    break\n",
    "            # Take action and update exploration to the newest value\n",
    "            kwargs = {}\n",
    "            if not param_noise:\n",
    "                update_eps = exploration.value(t)\n",
    "                update_param_noise_threshold = 0.\n",
    "            else:\n",
    "    \n",
    "                update_eps = 0.\n",
    "                # Compute the threshold such that the KL divergence between perturbed and non-perturbed\n",
    "                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).\n",
    "                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017\n",
    "                # for detailed explanation.\n",
    "                update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(num_actions))\n",
    "                kwargs['reset'] = reset\n",
    "                kwargs['update_param_noise_threshold'] = update_param_noise_threshold\n",
    "                kwargs['update_param_noise_scale'] = True\n",
    "            \n",
    "\n",
    "            action = act(np.array(obs), update_eps=update_eps, **kwargs)[0]\n",
    "            env_action = action\n",
    "            reset = False\n",
    "            \n",
    "             # Get information for plotting\n",
    "            vehicle_speed  = 100 * env_info.vector_observations[0][-8]\n",
    "            num_overtake   = env_info.vector_observations[0][-7]\n",
    "            num_lanechange = env_info.vector_observations[0][-6]\n",
    "            \n",
    "            \n",
    "            # Get information for update\n",
    "            env_info = env.step(action)[default_brain]\n",
    "            new_obs = get_obs_state_lidar(env_info)\n",
    "            rew = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            #new_obs, rew, done, _ = env.step(env_action)\n",
    "            # Store transition in the replay buffer.\n",
    "            replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "            obs = new_obs\n",
    "            \n",
    "            \n",
    "            episode_rewards[-1] += rew\n",
    "            num_episodes = len(episode_rewards)\n",
    "            \n",
    "            tab=episode_rewards[-print_freq-1:-1]\n",
    "            \n",
    "            if len(tab)==0:\n",
    "                mean_reward=episode_rewards[-1]\n",
    "            else:\n",
    "                mean_reward = round(np.mean(tab), 1) #mean reward of last print_freq episode\n",
    "            \n",
    "            speed_list.append(vehicle_speed)\n",
    "            overtake_list.append(num_overtake)\n",
    "            lanechange_list.append(num_lanechange)\n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            #print('step: ' + str(t) + ' / '  + 'episode: ' + str(num_episodes)+' action '+str(action)+' done '+str(done))\n",
    "            \n",
    "            if done:\n",
    "                #eps = tf.get_variable(\"eps\", ())\n",
    "                #Epsilon=eps.eval()\n",
    "                \n",
    "                # Print informations if terminal\n",
    "                print('step: ' + str(t) + ' / '  + 'episode: ' + str(num_episodes) + ' / ' + ' episode_rewards: ' \n",
    "                      + str(episode_rewards[-1]))\n",
    "                \n",
    "                env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "                reset = True\n",
    "                obs = get_obs_state_lidar(env_info)\n",
    "                #obs = env.reset()\n",
    "                episode_rewards.append(0.0)\n",
    "                #reset = True\n",
    "                \n",
    "                avg_speed = np.mean(speed_list)\n",
    "                avg_overtake = np.mean(overtake_list)\n",
    "                avg_lanechange = np.mean(lanechange_list)\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            if train_mode and t > learning_starts and t % train_freq == 0:\n",
    "                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "                if prioritized_replay:\n",
    "                    experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))\n",
    "                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience\n",
    "                else:\n",
    "                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)\n",
    "                    weights, batch_idxes = np.ones_like(rewards), None\n",
    "                    \n",
    "                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)\n",
    "                if prioritized_replay:\n",
    "                    new_priorities = np.abs(td_errors) + prioritized_replay_eps\n",
    "                    replay_buffer.update_priorities(batch_idxes, new_priorities)\n",
    "            \n",
    "                \n",
    "\n",
    "            if train_mode and t > learning_starts and t % target_network_update_freq == 0:\n",
    "                # Update target network periodically.\n",
    "                update_target()\n",
    "\n",
    "            \n",
    "            \n",
    "            if done and print_freq is not None and num_episodes % print_freq == 0:\n",
    "                #logger.record_tabular(\"steps\", t)\n",
    "                #logger.record_tabular(\"episodes\", num_episodes)\n",
    "                #logger.record_tabular(\"mean 100 episode reward\", mean_reward)\n",
    "                #logger.record_tabular(\"% time spent exploring\", int(100 * exploration.value(t)))\n",
    "                #logger.dump_tabular()\n",
    "                \n",
    "                \n",
    "                #avg_speed      = np.mean(speed_list)\n",
    "                #avg_overtake   = np.mean(overtake_list)\n",
    "                #avg_lanechange = np.mean(lanechange_list)\n",
    "                tensorboard_info = [avg_speed, avg_overtake, avg_lanechange,mean_reward]\n",
    "                \n",
    "                for i in range(len(tensorboard_info)):\n",
    "                    sess.run(update_ops[i], feed_dict = {summary_placeholders[i]: float(tensorboard_info[i])})\n",
    "                    \n",
    "                summary_str = sess.run(summary_op)\n",
    "                summary_writer.add_summary(summary_str, t)\n",
    "                \n",
    "                speed_list = []\n",
    "                overtake_list = []\n",
    "                lanechange_list = []\n",
    "                \n",
    "\n",
    "            if (train_mode and checkpoint_freq is not None and t > learning_starts and num_episodes >= print_freq and t % checkpoint_freq == 0):\n",
    "                \n",
    "                if saved_mean_reward is None or mean_reward > saved_mean_reward:\n",
    "                    \n",
    "                    if print_freq is not None:\n",
    "                        logger.log(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
    "                                   saved_mean_reward, mean_reward))\n",
    "                        \n",
    "                        print(\"Saving model due to mean reward increase: {} -> {}\".format(\n",
    "                                   saved_mean_reward, mean_reward))\n",
    "                    save_state(model_file)\n",
    "                    model_saved = True\n",
    "                    saved_mean_reward = mean_reward\n",
    "                    \n",
    "        if model_saved:\n",
    "            if print_freq is not None:\n",
    "                logger.log(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
    "                print(\"Restored model with mean reward: {}\".format(saved_mean_reward))\n",
    "                load_state(model_file)\n",
    "\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model =learn(env,dueling=False,checkpoint_path='saved_networks/',train_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
